# -*- coding: utf-8 -*-
"""KeywordsExtractionusingKmeansandKNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lpjzGR1ViCNAvrPVr7TkzB52er1DWePe
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from nltk.tokenize import word_tokenize
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from sklearn.model_selection import train_test_split
# Load the dataset
data = pd.read_csv("/content/sample_data/train_data.csv")

# Preprocessing - stopwords removal,tokenization and TF-IDF
def preprocess(Text):
    stopwords = nltk.corpus.stopwords.words('english')
    words = word_tokenize(Text)
    words = [word for word in words if word.isalpha()]
    words = [word for word in words if word not in stopwords]
    return " ".join(words)

data["Text"] = data["Text"].apply(preprocess)

vectorizer = TfidfVectorizer()
tfidf = vectorizer.fit_transform(data["Text"])
X_train, X_test, y_train, y_test = train_test_split(tfidf, data['Text'], test_size=0.2, random_state=0)

from sklearn.feature_extraction.text import TfidfVectorizer
k = 8

# Fit the k-means model on the preprocessed training data
kmeans = KMeans(n_clusters=k)
kmeans.fit(X_train)

# Assign each document in the training data to a cluster
train_clusters = kmeans.predict(X_train)

test_clusters = kmeans.predict(X_test)

# Compute the centroid vectors for each cluster
cluster_centers = kmeans.cluster_centers_

# Choose the number of keywords to extract for each cluster
n = 9

# Initialize a list to store the keywords for each cluster
cluster_keywords = []
import matplotlib.pyplot as plt
import seaborn as sns
order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names_out()
# Extract the keywords for each cluster
for i in range(k):
    # Compute the indices of the top n features for this cluster
    top_features_indices = np.argsort(cluster_centers[i])[-n:]

    # Extract the feature names (i.e., words) corresponding to these indices
    #cluster_keywords.append([vectorizer.get_feature_names()[index] for index in top_features_indices])
    cluster_keywords.append([vectorizer.get_feature_names_out()[index] for index in top_features_indices])
    print("Cluster %d:" % i)
    for ind in order_centroids[i, :10]:
        print(' %s' % terms[ind])
    print()
# Print the keywords for each cluster
for i, keywords in enumerate(cluster_keywords):
    print("Cluster ", i, " Keywords: ", keywords)

# Use KNN to classify the documents in the test data
knn = KNeighborsClassifier()
knn.fit(X_train, train_clusters)
test_predictions = knn.predict(X_test)

accuracy = accuracy_score(test_clusters, test_predictions)
precision = precision_score(test_clusters, test_predictions, average='weighted')
recall = recall_score(test_clusters, test_predictions, average='weighted')
f1 = f1_score(test_clusters, test_predictions, average='weighted')

# Print the results
print("Accuracy: ", accuracy)
print("Precision: ", precision)
print("Recall: ", recall)
print("F1-Score: ", f1)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train.toarray())
X_test = scaler.transform(X_test.toarray())

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X_train)
    wcss.append(kmeans.inertia_)

# Plot the elbow curve to determine the optimal number of clusters
plt.plot(range(1, 11), wcss)
plt.title("Elbow Curve")
plt.xlabel("Number of clusters")
plt.ylabel("WCSS")
plt.show()